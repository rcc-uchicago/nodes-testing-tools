#!/bin/bash

# submit:
#   sbatch --nodelist=midway3-0279,midway3-0280 queue-gpu-nodes.txt

#SBATCH --job-name=gpu-test
#SBATCH --account=rcc-staff
#SBATCH --partition=test

#SBATCH --reservation=Test_CPP   # System team would put the node under this reservation for testing purposes, or CS can create it

#SBATCH --ntasks-per-node=32
#SBATCH --gres=gpu:4             # gpu:2

#SBATCH --mem=0
#SBATCH --time=00:30:00
#SBATCH --exclusive

nodename=$SLURM_NODELIST

OUTPUT="output-gpu-$nodename.txt"

echo "Job ID: $SLURM_JOB_ID" > $OUTPUT
echo "Nodes = $nodename" >> $OUTPUT
echo "Job type: CPU-only" >> $OUTPUT
echo "Date: `date`" >> $OUTPUT

cd $SLURM_SUBMIT_DIR
CWD=`pwd`

nodes=$SLURM_NNODES
ppn=$SLURM_NTASKS_PER_NODE
n=$(( ppn * nodes ))

ulimit -l unlimited
ulimit -s unlimited


lscpu >> $OUTPUT
lscpu --extended >> $OUTPUT

cores=`grep "CPU(s):" $OUTPUT`
cpu_start=`grep "CPU NODE SOCKET CORE" $OUTPUT`

# Check the output of the last lscpu command if the first column (CPU) and the 4th column (CORE) are equal
#    if CORE > CPU, then hardware threading is enabled

awk -v cores="$cores" -v start="$cpu_start" 'BEGIN{hyperthreading=0;}{
  if (NR > start && NR < start+cores) {
    if ($1 != $4) hyperthreading=1;
  }
} END{ if (hyperthreading==1) printf("Hyperthreading is ON.");}' $OUTPUT

nvidia-smi >> $OUTPUT
num_gpus=`nvidia-smi --list-gpus | wc -l`

# Running CUDA sample vectorAdd (and GPU burn, optional)
module load cuda/12.2
gpu=`grep "NVIDIA" $OUTPUT  | grep "On" | tail -n1 | awk '{print $4}'`

# iterate through the GPUs
export GPU_BURN_PATH=/project/rcc/shared/nodes-testing/gpu-burn

for (( gpu_id=0; gpu_id < num_gpus; gpu_id++ ))
do
  export CUDA_VISIBLE_DEVICES=$gpu_id
  echo "testing GPU ID $gpu_id .." >> $OUTPUT

  # vectorAdd
  $CUDA_HOME/samples/bin/x86_64/linux/release/vectorAdd >> $OUTPUT

  # gpu_burn
  if [[ $gpu == *"A100"* ]]; then
    arch="sm80"
  elif [[ $gpu == *"A40"* ]]; then
    arch="sm86"
  elif [[ $gpu == *"L40"* ]]; then
    arch="sm89"
  elif [[ $gpu == *"H100"* ]]; then
    arch="sm90"
  elif [[ $gpu == *"V100"* ]]; then
    arch="sm70"
  fi
  echo "GPU arch = $arch" >> $OUTPUT
  #$GPU_BURN_PATH/gpu_burn_$arch -d 360  >> $OUTPUT
done

# make all the devices visible
echo "Number of GPUs per node requested: $num_gpus"
last_idx=$(( num_gpus - 1 ))
all_gpus=`seq -s ',' 0 $last_idx`
export CUDA_VISIBLE_DEVICES=$all_gpus

# LAMMPS is chosen because we can run with MPI on the whole node or across multiple nodes, 
# and memory consumption can be tuned by the parameter r below

# testing: 
#  1) loading modules from /software
#  2) accessing files from /project

if grep -q "AMD" $OUTPUT; then
  echo "$nodename has AMD CPUs" >> $OUTPUT
   module load mpich/3.4.3+gcc-10.2.0 cuda/12.2
   export PATH=/project/rcc/shared/nodes-testing/lammps/build:$PATH
else
  echo "$nodename has Intel CPUs" >> $OUTPUT
  module load lammps/29Aug2024
fi

echo "Running $(which lmp) with $n procs"  >> $CWD/$OUTPUT

cd /project/rcc/shared/nodes-testing/gpu_stability/lammps
# r controls the problem size, that is, the number of atoms simulated, the higher r, the more RAM needed per proc
r=16                 # 16 for 256 GB mem nodes, 24 for 1TB mem nodes, 35 for bigger mem nodes
# t controls how long the simulation takes
t=100

mpirun -np $n -bind-to core -map-by numa lmp_gpu -in in.lj -v x $r -v y $r -v z $r -v t $t -sf gpu -pk gpu $num_gpus >> $CWD/$OUTPUT

if [ -e $OUTPUT ]
then
  n=`grep "Loop time" $OUTPUT | wc -l`

  if [ $n -eq 1 ]
  then
    echo "PASSED"
  else
    echo "FAILED"
  fi
else
  echo "$OUTPUT does not exist"
fi

echo "Running PyTorch for neutral network training with GPU support ..."  >> $OUTPUT
cd /project/rcc/shared/nodes-testing/gpu_stability/pytorch/

module load python/miniforge-24.1.2
source /project/rcc/shared/nodes-testing/testing-env/bin/activate

for (( gpu_id = 0; gpu_id < num_gpus; gpu_id++ ))
do
   python test.py $gpu_id >> $CWD/$OUTPUT
done

deactivate
#module unload python/miniforge-24.1.2

# running Quantum Espresso with GPU support (optional)
running_qe=0
if [ $running_qe -eq 1 ]
then
  echo "Running Quantum Espresso with GPU acceleration ..."  >> $CWD/$OUTPUT
  cd /project/rcc/shared/nodes-testing/gpu_stability/qe/
  module purge
  module load rcc/default slurm/current
  module load espresso/7.4+nvhpc-24.1
  nvaccelinfo -v >> $CWD/$OUTPUT
  export NO_STOP_MESSAGE=1
  mpirun -np 8 -bind-to core -map-by core pw.x -input ausurf.in -npool 2 >> $CWD/$OUTPUT
fi

# running VASP with GPU support (optional)
running_vasp=0
if [ $running_vasp -eq 1 ]
then
  echo "Running VASP with GPU acceleration ..."  >> $CWD/$OUTPUT
  cd /project/rcc/shared/nodes-testing/gpu_stability/vasp/COF300mod1
  module purge
  module load rcc/default slurm/current
  module load vasp/6.4.0-gpu

  # 4 MPI procs for all the available GPUs
  mpirun -np 4 --mca  btl_openib_warn_no_device_params_found 0 --mca btl_openib_allow_ib true vasp_gam >> $CWD/$OUTPUT

  module unload vasp/6.4.0-gpu
fi

echo "Done"

cd $CWD

